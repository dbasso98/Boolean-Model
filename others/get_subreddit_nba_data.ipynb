{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "get-subreddit-nba-data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiUYVl0aXz7O"
      },
      "source": [
        "# Reddit API Data Scraping\n",
        "---\n",
        "\n",
        "In this notebook, I utilize Reddit's built in API .json functionality to scrape post data from four subreddits. I then export this data into .csv files to use in my analysis notebook. \n",
        "\n",
        "My chosen subreddits are as follows:\n",
        "\n",
        "- r/nba\n",
        "- r/nfl\n",
        "- r/cfb\n",
        "- r/CollegeBasketball\n",
        "\n",
        "I have taken mostly new posts from the subreddits, but I have also supplemented this with the top 500 posts from the past year into each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxiuu0YSXz7l"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import requests, re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAOu77iRYZeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f070fc-bdd6-4f4e-bbd3-b0119b466185"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPTOXAy4Xz7p"
      },
      "source": [
        "# update pandas global settings to view all columns and rows\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUj1pn8WXz7q"
      },
      "source": [
        "# import existing subreddit data\n",
        "nba_df = pd.read_csv('/content/drive/MyDrive/IR_Project/nba_subreddit_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "4cj9nhWSXz7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe0fa4b-89fc-48ed-9ed5-2ee8af40152e"
      },
      "source": [
        "# check shape of dataframes\n",
        "print(nba_df.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3011, 114)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkhUK0LGXz7t"
      },
      "source": [
        "# enter subreddit urls\n",
        "nba_url = 'https://www.reddit.com/r/nba.json'\n",
        "# establish our header\n",
        "header = {'User-agent': 'subreddit get requests'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hn1sHvpXz7v"
      },
      "source": [
        "# initial get request to test API\n",
        "res = requests.get(nba_url, headers=header)\n",
        "nba_res = res.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "G5Xlb31DXz7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16227fe-c980-4923-c808-6f0c34e752db"
      },
      "source": [
        "# check request status\n",
        "res.status_code"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "NmbydAFhXz7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b027dcf-dc2c-4950-f6b0-e4544de1a89b"
      },
      "source": [
        "# explore keys for test request\n",
        "nba_res['data']['children'][0]['data'].keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'is_created_from_ads_ui', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'gildings', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'all_awardings', 'awarders', 'media_only', 'link_flair_template_id', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'author_is_blocked', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SEn8eTbRXz71"
      },
      "source": [
        "# define function to get num pages of posts from a subreddit, start collecting at a defined after\n",
        "def reddit_scraper(url, num, after = None):\n",
        "    posts = []\n",
        "    # loop through the num pages, each subreddit .json returns 25 posts \n",
        "    for page in range(num):\n",
        "        # initiate params modifier for posts if there no defined after\n",
        "        if after == None:\n",
        "            params = {}\n",
        "        # add in after id for each loop following to ensure no duplicate posts\n",
        "        else:\n",
        "            params = {'after': after}\n",
        "        # call our get request for the posts\n",
        "        res = requests.get(url, params=params, headers=header)\n",
        "        # check status code, 200 means posts were successfully downloaded\n",
        "        if res.status_code == 200:\n",
        "            # convert request to .json\n",
        "            new_json = res.json()\n",
        "            # extend list from the 'children' dictionary for each request\n",
        "            posts.extend(new_json['data']['children'])\n",
        "            # update after id\n",
        "            after = new_json['data']['after']\n",
        "        else:\n",
        "            # print status code if not 200\n",
        "            print(res.status_code)\n",
        "            break\n",
        "        # wait 1 second\n",
        "        time.sleep(1)\n",
        "        \n",
        "    # create a new dataframe with the 'data' from each post\n",
        "    new_df = pd.DataFrame([post['data'] for post in posts])\n",
        "    \n",
        "    # print final value of after\n",
        "    print(f'Final value of after parameter: {after}')\n",
        "    \n",
        "    # return the dataframe\n",
        "    return new_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGGJSyg5Xz74"
      },
      "source": [
        "## Data from r/nba\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zFjGr6yNXz76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a63ab4c-05e6-4679-d753-d38316f53221"
      },
      "source": [
        "# call subreddit scraping function\n",
        "new_nba_df = reddit_scraper(nba_url, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final value of after parameter: t3_r7q16a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "D95kZIU9Xz79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e0c91c1-eee8-48c2-d889-d306404f8e02"
      },
      "source": [
        "# check shape of scraped dataframe\n",
        "new_nba_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(252, 112)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "wMGF06xoXz8B"
      },
      "source": [
        "new_nba_df = pd.concat([nba_df, new_nba_df], axis=0, sort=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JEmc3RzvXz8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2200404e-4df0-4825-d4f1-fb08d0ff04ca"
      },
      "source": [
        "# confirm concatenation\n",
        "new_nba_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3011, 114)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWz1HmLUXz8C"
      },
      "source": [
        "# reset index\n",
        "new_nba_df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W4u_zpWXz8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0979dcbb-52a5-488d-e26e-bca954431d88"
      },
      "source": [
        "# count number of unique posts\n",
        "new_nba_df['name'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2314"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K15TvTRXz8E"
      },
      "source": [
        "# export CSV of original and new data\n",
        "new_nba_df.to_csv(\"/content/drive/MyDrive/IR_Project/nba_subreddit_data.csv\", index=False)\n",
        "nba_df.to_csv(\"/content/drive/MyDrive/IR_Project/nba_subreddit_data - backup.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WA-v740eNf9"
      },
      "source": [
        "nba_df = pd.read_csv('/content/drive/MyDrive/IR_Project/nba_subreddit_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vno5hdzZ7rHJ"
      },
      "source": [
        "def combine_text(df, cols):\n",
        "    for col in cols:\n",
        "        df[col] = df[col].fillna(value = \"\")\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLGSgaJ67vAz"
      },
      "source": [
        "nba_df = combine_text(nba_df, [['title', 'selftext']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNPkra_xdBoF"
      },
      "source": [
        "# reshape dataframes to only include text, name, and subreddit columns\n",
        "nba_df = nba_df[['title', 'selftext','name']]\n",
        "# drop duplicate posts based on the post name ID\n",
        "nba_df = nba_df.drop_duplicates(subset=['name'])\n",
        "\n",
        "# define function to run same regex over a dataframe\n",
        "def sub_preprocess(sub):\n",
        "    # run regex to remove urls\n",
        "    sub['selftext'] = sub['selftext'].map(lambda x: re.sub(r\"((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?\", ' ', x))\n",
        "\n",
        "    # run regex to remove certain characters\n",
        "    sub['selftext'] = sub['selftext'].map(lambda x: re.sub(r'[^\\w^\\s^-^\\$]',' ',x))\n",
        "    sub['selftext'] = sub['selftext'].map(lambda x: re.sub(r\"[@\\?\\.%_\\[\\]()+-:*\\\"]\", ' ', x, flags=re.I))\n",
        "    sub['selftext'] = sub['selftext'].map(lambda x: re.sub(r\"[,']\", '', x, flags=re.I))\n",
        "    sub['selftext'] = sub['selftext'].map(lambda x: re.sub(\"(?<![\\w'])\\w+?(?=\\b|'s)\", ' ', x))\n",
        "\n",
        "    # run regex to remove line breaks and tabs\n",
        "    sub['selftext'] = sub['selftext'].map(lambda x: re.sub(r\"\\s+\", ' ', x))\n",
        "\n",
        "    # run regex to remove common words\n",
        "    sub['selftext'] = sub['selftext'].map(lambda x: re.sub('(nfl|nba|college|ncaa|team|game|season|year|player|thread|just|like|time|new|s)[s]?', ' ', x,  flags=re.I))\n",
        "\n",
        "sub_preprocess(nba_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "V00xNn6leTEL",
        "outputId": "d55635d3-bc93-4141-813a-57cf1774f48b"
      },
      "source": [
        "nba_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>selftext</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Game Threads Index + Daily Discussion (July 04...</td>\n",
              "      <td>Today   Tip off Away Home GDT PGT Ye terday  ...</td>\n",
              "      <td>t3_c93mdd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019 NBA Free Agent Tracker</td>\n",
              "      <td>After hock Day Contract  ource Old Kawhi Leon...</td>\n",
              "      <td>t3_c6ffge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Haynes] Free agent guard Quinn Cook has reach...</td>\n",
              "      <td></td>\n",
              "      <td>t3_c9trsl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Wojnarowski] Presti pursued a package of Russ...</td>\n",
              "      <td></td>\n",
              "      <td>t3_c9tebd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(Shelburne in ESPN piece) Still, Leonard's rec...</td>\n",
              "      <td>Another intere ting tidbit Then Durant got a ...</td>\n",
              "      <td>t3_c9tph1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  \\\n",
              "0  Game Threads Index + Daily Discussion (July 04...   \n",
              "1                        2019 NBA Free Agent Tracker   \n",
              "2  [Haynes] Free agent guard Quinn Cook has reach...   \n",
              "3  [Wojnarowski] Presti pursued a package of Russ...   \n",
              "4  (Shelburne in ESPN piece) Still, Leonard's rec...   \n",
              "\n",
              "                                            selftext       name  \n",
              "0   Today   Tip off Away Home GDT PGT Ye terday  ...  t3_c93mdd  \n",
              "1   After hock Day Contract  ource Old Kawhi Leon...  t3_c6ffge  \n",
              "2                                                     t3_c9trsl  \n",
              "3                                                     t3_c9tebd  \n",
              "4   Another intere ting tidbit Then Durant got a ...  t3_c9tph1  "
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgIeGBrZ0bU7"
      },
      "source": [
        "nba_df.to_csv(\"/content/drive/MyDrive/IR_Project/nba_subreddit_data_prep.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}